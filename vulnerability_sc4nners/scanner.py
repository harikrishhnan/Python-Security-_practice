#!/usr/bin/env python

import requests
import re
import urlparse
from BeautifulSoup import BeautifulSoup

class Scanner:
    def __init__(self, url, ignore_links):
        self.session = requests.Session()
        self.target_url = url
        self.target_links = []
        self.links_to_ignore = ignore_links

    def extract_links_from(self, url):
        response = self.session.get(url)
        # Use PYTHEX to test the REGEX
        # Returns all the matches that has href="anything"
        return re.findall('(?:href=")(.*?)"', response.text)  # 1st part just to find; 2nd part to return
        # response.content gets the content i.e page source of the webpage

    def crawl(self, url=None):
        if url == None:
            url = self.target_url
        href_links = self.extract_links_from(url)
        for link in href_links:
            # joining left alone path with the original url. like /path.. http://url/path
            link = urlparse.urljoin(url, link)

            # if # thats referencing is cut out
            if "#" in link:
                link = link.split("#")[0]


            # if its no already in the list then add, and not out of bound
            if self.target_url in link and link not in self.target_links and link not in self.links_to_ignore:
                self.target_links.append(link)
                print(link)
                self.crawl(link)  # recursively calling to spider

    def extract_forms(self, url):
        response = self.session.get(url)
        # BeautifulSoup is used for parsing HTML and XML documents, useful for web scraping
        parsed_html = BeautifulSoup(response.content)
        # findAll is used to find element in the html
        return parsed_html.findAll("form")

    def submit_form(self, form, value, url):
        # gets the value of action attribute (of form)
        action = form.get("action")
        # urljoin is used to join 2 urls
        post_url = urlparse.urljoin(url, action)
        # gets the value of method attribute (of form)
        method = form.get("method")
        # Since input is an element we need to loop once again
        inputs_list = form.findAll("input")
        post_data = {}
        for input in inputs_list:
            # gets the value of name attribute (of input)
            input_name = input.get("name")
            input_type = input.get("type")
            input_value = input.get("value")
            # Changing the value only if the input type is text
            if input_type == "text":
                input_value = value

            post_data[input_name] = input_value
        if method=="post":
            return self.session.post(post_url, data=post_data)
        return self.session.get(post_url, params=post_data)

    def run_scanner(self):
        for link in self.target_links:
            forms = self.extract_forms(link)
            for form in forms:
                print("[+] Testing form in " + link)
                is_vulnerable_to_xss = self.test_xss_in_form(form, link)
                if is_vulnerable_to_xss:
                    print("[***] XSS DISCOVERED IN " + link + " in the following form")
                    print(form)

            if "=" in link:
                print("[+] Testing " + link)
                is_vulnerable_to_xss = self.test_xss_in_link(link)
                if is_vulnerable_to_xss:
                    print("[***] XSS DISCOVERED IN " + link)

    def test_xss_in_link(self, url):
        xss_test_script = "<scRipt>alert('test')</sCript>"
        url = url.replace("=", "=" + xss_test_script)
        response = self.session.get(url)
        return xss_test_script in response.content

    def test_xss_in_form(self, form, url):
        xss_test_script = "<scRipt>alert('test')</sCript>"
        response = self.submit_form(form, xss_test_script, url)
        return xss_test_script in response.content